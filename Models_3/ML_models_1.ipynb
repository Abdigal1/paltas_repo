{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "#from skimage import io\n",
    "#import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "#from meta_load import *\n",
    "\n",
    "sys.path.append(os.path.join(\"..\",'Data_prep'))\n",
    "sys.path.append(os.path.join(\"..\",'Models2'))\n",
    "\n",
    "from Custom_dataloader import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "DB=\"/run/user/1000/gvfs/afp-volume:host=MyCloudPR4100.local,user=aorus_2,volume=Paltas_DataBase\"\n",
    "Dbd=\"/run/user/1000/gvfs/afp-volume:host=MyCloudPR4100.local,user=aorus_2,volume=Paltas_DataBase/Data_Base\"\n",
    "#DBd=\"//MYCLOUDPR4100/Paltas_DataBase/Data_Base\"\n",
    "#DB=\"//MYCLOUDPR4100/Paltas_DataBase\"\n",
    "#d_t=transforms.Compose([phantom_segmentation(False)])\n",
    "\n",
    "datar=Dataset_direct(root_dir=Dbd,ImType=['PhantomRGB'],Intersec=False,retrieve_img=False)\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(os.path.join(DB,\"metadata\"))\n",
    "meta=['bl_per_wmask_phantom','stat_val_hsv_wmask_phantom','stat_val_lab_wmask_phantom']\n",
    "ddlist=np.array(os.listdir(os.path.join(DB,\"metadata\",meta[0])))\n",
    "dlist=np.vectorize(lambda d:d.split(\".\")[0])(ddlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date=np.vectorize(lambda d:(\"_\").join(datar[np.where(datar.aID==d)[0][0]][\"Date\"].split(\"_\")[:2]))(dlist)\n",
    "Y=np.vectorize(lambda d:datar[np.where(datar.aID==d)[0][0]][\"landmarks\"])(dlist)\n",
    "timei=np.vectorize(lambda d:datar[np.where(datar.aID==d)[0][0]]['PhantomRGB_metadata']['DateTime'])(dlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat=np.vectorize(lambda d:(pd.read_pickle(os.path.join(DB,\"metadata\",meta[0],d)))[d.split('.')[0]])(ddlist).reshape(-1,1)\n",
    "camp=np.array(['mean','std','mode'])\n",
    "dat1=np.vectorize(lambda d,c:(pd.read_pickle(os.path.join(DB,\"metadata\",meta[1],d)))[d.split('.')[0]][c],\n",
    "            signature=\"(),()->()\")(ddlist.reshape(-1,1),camp)\n",
    "dat2=np.vectorize(lambda d,c:(pd.read_pickle(os.path.join(DB,\"metadata\",meta[2],d)))[d.split('.')[0]][c],\n",
    "            signature=\"(),()->()\")(ddlist.reshape(-1,1),camp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date=np.vectorize(lambda d:(\"_\").join(datar[np.where(datar.aID==d)[0][0]][\"Date\"].split(\"_\")[:2]))(dlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE GEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_dir=\"/run/user/1000/gvfs/afp-volume:host=MyCloudPR4100.local,user=aorus_2,volume=Paltas_DataBase/metadata_VAE_v2\"\n",
    "metad=np.array(os.listdir(meta_dir)[1:])\n",
    "metad_dict=np.vectorize(lambda meta: pickle.load(open(os.path.join(meta_dir,meta),'rb')))(metad)\n",
    "meta_u=np.vectorize(lambda dic:dic['u'],signature='()->(j)')(metad_dict)\n",
    "meta_sig=np.vectorize(lambda dic:dic['sig'],signature='()->(j)')(metad_dict)\n",
    "meta_C=np.vectorize(lambda dic:dic['Class'][0],otypes=[object],signature='()->()')(metad_dict)\n",
    "meta_D=np.vectorize(lambda dic:dic['Date'][0],otypes=[object],signature='()->()')(metad_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07869902, 0.        , 0.14296825, ..., 0.35693416, 0.        ,\n",
       "        0.13558303],\n",
       "       [0.60876954, 0.        , 0.        , ..., 0.82567316, 0.        ,\n",
       "        0.91215366],\n",
       "       [0.68633395, 0.        , 0.        , ..., 0.9090571 , 0.        ,\n",
       "        0.9943594 ],\n",
       "       ...,\n",
       "       [0.24487375, 0.55729955, 0.63203096, ..., 0.        , 0.6529889 ,\n",
       "        0.        ],\n",
       "       [0.41070902, 0.06070173, 0.08585122, ..., 0.36066654, 0.07279721,\n",
       "        0.18247795],\n",
       "       [0.5702767 , 0.13581806, 0.38246447, ..., 0.4871438 , 0.27914578,\n",
       "        0.24046168]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafd={\n",
    "    'ID':dlist,\n",
    "    'lab_mean':dat2[:,0],\n",
    "    'lab_std':dat2[:,1],\n",
    "    'lab_mode':dat2[:,2],\n",
    "    'hsv_mean':dat1[:,0],\n",
    "    'hsv_std':dat1[:,1],\n",
    "    'hsv_mode':dat1[:,2],\n",
    "    'black_per':dat.reshape(-1,),\n",
    "    'hour':np.vectorize(lambda cdt:cdt.split(' ')[1].split(':')[0])(timei),\n",
    "    'date':date,\n",
    "    'Y':Y\n",
    "}\n",
    "data=pd.DataFrame(datafd)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('/home/liarpi2/Escritorio/metadata_VAE_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datar[0]['PhantomRGB_metadata']['DateTime'])\n",
    "print(datar[5]['PhantomRGB_metadata']['DateTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "cont=['lab_mean','lab_std','lab_mode','hsv_mean','hsv_std','hsv_mode','black_per']\n",
    "features = data[cont]\n",
    "\n",
    "\n",
    "scaler = StandardScaler().fit(features.values)\n",
    "features = scaler.transform(features.values)\n",
    "data[cont]=features\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode time and day\n",
    "data=pd.concat([data,pd.get_dummies(data['hour'],columns='hour'),pd.get_dummies(data['date'],columns='date')],axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_cl=np.unique(np.array(data['hour']))\n",
    "print(hour_cl)\n",
    "day_cl=np.unique(np.array(data['date']))\n",
    "print(day_cl)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clasificación para detección de deficiencia hidrica\n",
    "data=data[(data['Y']=='Control')|\\\n",
    "          #(data['Y']=='K_Control')|\\\n",
    "          #(data['Y']=='N_Control')|\\\n",
    "          #(data['Y']=='P_Control')|\\\n",
    "          (data['Y']=='H50%')\\\n",
    "           #(data['Y']=='H75%')\\\n",
    "         ]\n",
    "#X=data[list(data.columns[1:7]) + list(data.columns[16:])]\n",
    "X=data[list(data.columns[1:7])]\n",
    "print(X.columns)\n",
    "Yo=((data['Y']=='Control') | (data['Y']=='K_Control') | (data['Y']=='N_Control') | (data['Y']=='P_Control')).astype(int)\n",
    "Yo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "GR=np.arange(1,6)\n",
    "def LR_GR(GR,X,Y):\n",
    "    polynomial_features = PolynomialFeatures(degree=GR)\n",
    "    r=polynomial_features.fit_transform(X[:,:6])\n",
    "    Xco=np.hstack((r,np.array(X[:,6:])))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xco, Y, random_state=20,train_size=0.8)\n",
    "    Lr1=LogisticRegression(max_iter=4000,C=5)\n",
    "    Lr1.fit(X_train,y_train)\n",
    "    pr_tr=Lr1.score(X_train,y_train)\n",
    "    pr_ts=Lr1.score(X_test,y_test)\n",
    "    print(GR)\n",
    "    print(' R^2_train: ', pr_tr)\n",
    "    print(' R^2_test: ', pr_ts)\n",
    "    return Lr1,GR,pr_tr,pr_ts\n",
    "v_LR_GR=np.vectorize(LR_GR,signature='(),(j,k),(l)->(),(),(),()')\n",
    "LR,GR,pr_tr2,pr_ts2=v_LR_GR(GR,X,np.array(Yo))\n",
    "\n",
    "plt.plot(GR,pr_tr2, 'ro--')\n",
    "plt.plot(GR,pr_ts2, 'bo--')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(np.arange(LR[0].coef_[0].shape[0]),np.abs(LR[0].coef_[0]))\n",
    "plt.show()\n",
    "\n",
    "plt.bar(np.arange(LR[0].coef_[0].shape[0]),np.abs(LR[0].coef_[0].sort()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_features = PolynomialFeatures(degree=1)\n",
    "Xc=polynomial_features.fit_transform(X[X.columns[:6]])\n",
    "Xc=np.hstack((X[X.columns[6:]],Xc))\n",
    "C=np.arange(0.001,3,0.1)\n",
    "def LR_C(C,X,Y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=20,train_size=0.8)\n",
    "    Lr1=LogisticRegression(max_iter=4000,C=C)\n",
    "    Lr1.fit(X_train,y_train)\n",
    "    pr_tr=Lr1.score(X_train,y_train)\n",
    "    pr_ts=Lr1.score(X_test,y_test)\n",
    "    print(C)\n",
    "    print(' R^2_train: ', pr_tr)\n",
    "    print(' R^2_test: ', pr_ts)\n",
    "    return Lr1,C,pr_tr,pr_ts\n",
    "v_LR_C=np.vectorize(LR_C,signature='(),(j,k),(l)->(),(),(),()')\n",
    "\n",
    "LR,C,pr_tr1,pr_ts1=v_LR_C(C,Xc,np.array(Yo))\n",
    "\n",
    "plt.plot(C,pr_tr1, 'ro--')\n",
    "plt.plot(C,pr_ts1, 'bo--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C[np.argmax(pr_ts1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.hstack((np.array(polynomial_features.get_feature_names(X.columns[:6])),np.array(X.columns[6:])))[np.abs(LR[0].coef_[0])>0.2]\n",
    "np.hstack((np.array(polynomial_features.get_feature_names(X.columns[:6]))))[np.abs(LR[0].coef_[0])>0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b1aad70b8260>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mDP\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmSL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mMSL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DP=np.arange(4,30)\n",
    "mSL=np.arange(1,70,2)\n",
    "MSL=np.arange(2,25,1)\n",
    "\n",
    "#VARIANDO DP\n",
    "def DT_DP(DP,X,Y):\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=20,train_size=0.8)\n",
    "  Lr1 = DecisionTreeClassifier(max_depth=DP,min_samples_leaf=21,max_leaf_nodes=6)#-----------------------------------------\n",
    "  #Lr1 = DecisionTreeClassifier(max_depth=DP,min_samples_leaf=8)\n",
    "  Lr1.fit(X_train,y_train)\n",
    "  pr_tr=Lr1.score(X_train,y_train)\n",
    "  pr_ts=Lr1.score(X_test,y_test)\n",
    "  print(DP)\n",
    "  print(' R^2_train: ', pr_tr)\n",
    "  print(' R^2_test: ', pr_ts)\n",
    "  return Lr1,DP,pr_tr,pr_ts\n",
    "v_DT_DP=np.vectorize(DT_DP,signature='(),(j,k),(l)->(),(),(),()')\n",
    "\n",
    "DT1,DP,pr_tr1,pr_ts1=v_DT_DP(DP,X,np.array(Yo))\n",
    "\n",
    "#VARIANDO mSL\n",
    "def DT_mSL(mSL,X,Y):\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=20,train_size=0.8)\n",
    "  Lr1 = DecisionTreeClassifier(max_depth=5,min_samples_leaf=mSL,max_leaf_nodes=6)#-----------------------------------------\n",
    "  #Lr1 = DecisionTreeClassifier(min_samples_leaf=mSL)\n",
    "  Lr1.fit(X_train,y_train)\n",
    "  pr_tr=Lr1.score(X_train,y_train)\n",
    "  pr_ts=Lr1.score(X_test,y_test)\n",
    "  print(mSL)\n",
    "  print(' R^2_train: ', pr_tr)\n",
    "  print(' R^2_test: ', pr_ts)\n",
    "  return Lr1,mSL,pr_tr,pr_ts\n",
    "v_DT_mSL=np.vectorize(DT_mSL,signature='(),(j,k),(l)->(),(),(),()')\n",
    "\n",
    "DT2,mSL,pr_tr2,pr_ts2=v_DT_mSL(mSL,X,np.array(Yo))\n",
    "\n",
    "#VARIANDO MSL\n",
    "def DT_MSL(MSL,X,Y):\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=20,train_size=0.8)\n",
    "  Lr1 = DecisionTreeClassifier(max_depth=5,min_samples_leaf=21,max_leaf_nodes=MSL)#-----------------------------------------\n",
    "  #Lr1 = DecisionTreeClassifier(max_leaf_nodes=MSL,min_samples_leaf=8)\n",
    "  Lr1.fit(X_train,y_train)\n",
    "  pr_tr=Lr1.score(X_train,y_train)\n",
    "  pr_ts=Lr1.score(X_test,y_test)\n",
    "  print(MSL)\n",
    "  print(' R^2_train: ', pr_tr)\n",
    "  print(' R^2_test: ', pr_ts)\n",
    "  return Lr1,MSL,pr_tr,pr_ts\n",
    "v_DT_MSL=np.vectorize(DT_MSL,signature='(),(j,k),(l)->(),(),(),()')\n",
    "\n",
    "DT3,MSL,pr_tr3,pr_ts3=v_DT_MSL(MSL,X,np.array(Yo))\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(DP,pr_tr1, 'ro--')\n",
    "plt.plot(DP,pr_ts1, 'bo--')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(mSL,pr_tr2, 'ro--')\n",
    "plt.plot(mSL,pr_ts2, 'bo--')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(MSL,pr_tr3, 'ro--')\n",
    "plt.plot(MSL,pr_ts3, 'bo--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT3[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sklearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c4df2df4b9bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDT3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sklearn' is not defined"
     ]
    }
   ],
   "source": [
    "sklearn.tree.plot_tree(DT3[6],feature_names=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=load_meta_v2()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
